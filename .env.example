# Travel Planner Configuration Template
# Copy this file to .env and fill in your actual values

# ============================================================================
# OPENAI LLM CONFIGURATION (Required for itinerary generation)
# ============================================================================

# OpenAI API Key (REQUIRED)
# Get your key from: https://platform.openai.com/api-keys
LLM__OPENAI_API_KEY=sk-your-api-key-here

# LangSmith API Key (Optional)
# For debugging and tracing LLM calls
# Get your key from: https://smith.langchain.com
LLM__LANGSMITH_API_KEY=

# Anthropic API Key (Optional)
# For Claude models
LLM__ANTHROPIC_API_KEY=

# Model Configuration
# Model to use for LLM calls
LLM__MODEL_NAME=gpt-4-turbo-preview

# Temperature for generation (0.0-2.0)
# Lower = more deterministic, Higher = more creative
LLM__TEMPERATURE=0.7

# Maximum tokens in response
LLM__MAX_TOKENS=2000

# Top-p sampling (0.0-1.0)
LLM__TOP_P=1.0

# Frequency penalty (-2.0 to 2.0)
LLM__FREQUENCY_PENALTY=0.0

# Presence penalty (-2.0 to 2.0)
LLM__PRESENCE_PENALTY=0.0

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LLM__LOG_LEVEL=INFO

# Log format
LLM__LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# ============================================================================
# CACHE CONFIGURATION (Phase 4)
# ============================================================================

# Enable response caching
LLM__CACHE_ENABLED=false

# Cache TTL in seconds (1 hour = 3600)
LLM__CACHE_TTL=3600

# ============================================================================
# COST TRACKING (Phase 2)
# ============================================================================

# Track token usage for cost monitoring
LLM__TRACK_TOKEN_USAGE=true

# ============================================================================
# TIMEOUTS & RETRIES
# ============================================================================

# Timeout for LLM calls in seconds
LLM__TIMEOUT_SECONDS=60

# Maximum retry attempts
LLM__MAX_RETRIES=3

# ============================================================================
# API CONFIGURATION
# ============================================================================

# API server host
API__HOST=0.0.0.0

# API server port
API__PORT=8000

# Secret key for JWT (MUST change in production)
API__SECRET_KEY=change-me-in-production

# ============================================================================
# APPLICATION CONFIGURATION
# ============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# Debug mode
DEBUG=false

# ============================================================================
# AWS CONFIGURATION
# ============================================================================

# AWS Region
AWS__REGION=us-east-1

# AWS Bedrock Model ID
AWS__BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# DynamoDB table names
DB__AGENTS_TABLE=langgraph-agents
DB__CHECKPOINTS_TABLE=langgraph-checkpoints
DB__CACHE_TABLE=langgraph-cache

# ============================================================================
# LANGGRAPH CONFIGURATION
# ============================================================================

# Checkpoint backend: dynamodb, memory, sqlite
LANGGRAPH__CHECKPOINT_BACKEND=dynamodb

# Maximum iterations per execution
LANGGRAPH__MAX_ITERATIONS=25

# Maximum execution time in seconds
LANGGRAPH__MAX_EXECUTION_TIME_SECONDS=300

# ============================================================================
# OBSERVABILITY
# ============================================================================

# Log level for observability
OBSERVABILITY__LOG_LEVEL=INFO

# Enable tracing
OBSERVABILITY__TRACING_ENABLED=true

# ============================================================================
# FEATURE FLAGS
# ============================================================================

# Enable RAG pipeline
FEATURE__ENABLE_RAG=true

# Enable caching
FEATURE__ENABLE_CACHING=true

# Enable streaming
FEATURE__ENABLE_STREAMING=true

